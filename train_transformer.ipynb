{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from einops import rearrange, repeat\n",
    "from ToApps.to_apps import to_slack\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import logical_not\n",
    "from ktfuncs import kt_transformer\n",
    "from builderfuncs import restore_model, EarlyStopAndSave\n",
    "from builderfuncs import build_transformer, save_whole_model, restore_model\n",
    "from parameterdicts import TransformerParameters\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/scaled_U2_data.csv', index_col=0)\n",
    "df.drop(\"UNNAMED: 0\", axis=1, inplace=True)\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(ds: tf.data.Dataset, ds_size: int, \n",
    "              train_split: float = 0.8, \n",
    "              val_split: float = 0.1, \n",
    "              test_split: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "                                                tf.data.Dataset, \n",
    "                                                tf.data.Dataset]:\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "   \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def mask_window(features: Tuple[Dict[tf.Tensor, tf.Tensor], tf.Tensor], \n",
    "                mask: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor], \n",
    "                                          tf.Tensor]:\n",
    "    inputs = features[0]\n",
    "    labels = features[1]\n",
    "    decoder_labels = tf.einsum('tf,t->tf', labels, mask)\n",
    "    mask = repeat(mask, 'i -> 1 i j', j=labels.shape[-2])\n",
    "    mask = tf.math.minimum(mask, rearrange(mask, '1 i j -> 1 j i'))\n",
    "    decoder_labels.set_shape([12, None])\n",
    "    mask.set_shape([None, 12, 12])\n",
    "    inputs.update({'decoder_labels':decoder_labels,\n",
    "                   'attention_mask':mask})\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def split_window(features: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor],\n",
    "                                               tf.Tensor]:\n",
    "    encoder_input = features[:12, 1:]\n",
    "    decoder_input = features[11:, 1:75]\n",
    "    labels = rearrange(features[11:, 0], 'a -> a 1')\n",
    "    encoder_input.set_shape([12, 247])\n",
    "    decoder_input.set_shape([12, 74])\n",
    "    labels.set_shape([12, None])\n",
    "    \n",
    "\n",
    "    return {'encoder_inputs':encoder_input, 'decoder_inputs':decoder_input}, labels\n",
    "\n",
    "                        \n",
    "def make_dataset(data: pd.DataFrame, length: int, \n",
    "                 batch_size: int = 64, multistep: bool = True) -> Tuple[tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset]:\n",
    "    data = np.array(data.iloc[:, :], dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                        targets=None,\n",
    "                                                        sequence_length=length,\n",
    "                                                        sequence_stride=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=1,\n",
    "                                                        batch_size=None)\n",
    "\n",
    "    ds = ds.filter(lambda x: tf.reduce_all(logical_not(tf.math.is_nan(x))))\n",
    "    ds = ds.map(split_window)\n",
    "    if multistep == True:\n",
    "        nums = np.zeros(170680)\n",
    "    else:\n",
    "        nums = np.random.randint(0, length/2-1, 170680)\n",
    "    mask = tf.sequence_mask(nums, length/2, dtype=tf.float32)\n",
    "    maskds = tf.data.Dataset.from_tensor_slices(mask)\n",
    "    ds = tf.data.Dataset.zip((ds, maskds))\n",
    "    ds = ds.map(mask_window).batch(batch_size)\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(170680//batch_size + 1))\n",
    "    train_ds, val_ds, test_ds = ttv_split(ds, 170680//batch_size, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "    train_ds = train_ds\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "t_tensorboard_callback = tf.keras.callbacks.TensorBoard(t_logdir, histogram_freq=1)\n",
    "t_earlystopper = EarlyStopAndSave(filepath=\"model_folder/transformer_folder\", patience=15, quickstop=\"val_r2\")\n",
    "train_data, val_data, test_data = make_dataset(df, 23, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from directory\\transformer_trial_1\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(kt_transformer,\n",
    "                                objective=kt.Objective('val_loss', 'min'),\n",
    "                                max_trials=200,\n",
    "                                directory='directory',\n",
    "                                project_name=\"transformer_trial_1\",\n",
    "                                seed=1)\n",
    "\n",
    "parameters = tuner.get_best_hyperparameters()[0].values\n",
    "parameters['look_back'] = parameters.pop('lookback')\n",
    "parameters['key_dim'] = 248//parameters['num_heads']\n",
    "model = build_transformer(parameters=parameters, name=\"transformer_model\", dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2132/2132 - 217s - loss: 0.0097 - r2: 0.7973 - val_loss: 0.0046 - val_r2: 0.9087 - 217s/epoch - 102ms/step\n",
      "Epoch 2/200\n",
      "2132/2132 - 236s - loss: 0.0051 - r2: 0.8989 - val_loss: 0.0040 - val_r2: 0.9219 - 236s/epoch - 111ms/step\n",
      "Epoch 3/200\n",
      "2132/2132 - 196s - loss: 0.0043 - r2: 0.9172 - val_loss: 0.0037 - val_r2: 0.9275 - 196s/epoch - 92ms/step\n",
      "Epoch 4/200\n",
      "2132/2132 - 205s - loss: 0.0037 - r2: 0.9292 - val_loss: 0.0027 - val_r2: 0.9510 - 205s/epoch - 96ms/step\n",
      "Epoch 5/200\n",
      "2132/2132 - 208s - loss: 0.0033 - r2: 0.9380 - val_loss: 0.0022 - val_r2: 0.9596 - 208s/epoch - 98ms/step\n",
      "Epoch 6/200\n",
      "wait: 1\n",
      "2132/2132 - 205s - loss: 0.0030 - r2: 0.9440 - val_loss: 0.0022 - val_r2: 0.9606 - 205s/epoch - 96ms/step\n",
      "Epoch 7/200\n",
      "2132/2132 - 203s - loss: 0.0028 - r2: 0.9493 - val_loss: 0.0019 - val_r2: 0.9651 - 203s/epoch - 95ms/step\n",
      "Epoch 8/200\n",
      "2132/2132 - 214s - loss: 0.0026 - r2: 0.9524 - val_loss: 0.0017 - val_r2: 0.9687 - 214s/epoch - 100ms/step\n",
      "Epoch 9/200\n",
      "2132/2132 - 232s - loss: 0.0024 - r2: 0.9554 - val_loss: 0.0017 - val_r2: 0.9705 - 232s/epoch - 109ms/step\n",
      "Epoch 10/200\n",
      "wait: 1\n",
      "2132/2132 - 218s - loss: 0.0024 - r2: 0.9572 - val_loss: 0.0019 - val_r2: 0.9653 - 218s/epoch - 102ms/step\n",
      "Epoch 11/200\n",
      "2132/2132 - 218s - loss: 0.0023 - r2: 0.9585 - val_loss: 0.0016 - val_r2: 0.9712 - 218s/epoch - 102ms/step\n",
      "Epoch 12/200\n",
      "2132/2132 - 197s - loss: 0.0022 - r2: 0.9607 - val_loss: 0.0015 - val_r2: 0.9735 - 197s/epoch - 92ms/step\n",
      "Epoch 13/200\n",
      "wait: 1\n",
      "2132/2132 - 209s - loss: 0.0021 - r2: 0.9620 - val_loss: 0.0017 - val_r2: 0.9688 - 209s/epoch - 98ms/step\n",
      "Epoch 14/200\n",
      "wait: 2\n",
      "2132/2132 - 214s - loss: 0.0021 - r2: 0.9630 - val_loss: 0.0018 - val_r2: 0.9678 - 214s/epoch - 101ms/step\n",
      "Epoch 15/200\n",
      "2132/2132 - 216s - loss: 0.0020 - r2: 0.9640 - val_loss: 0.0014 - val_r2: 0.9741 - 216s/epoch - 101ms/step\n",
      "Epoch 16/200\n",
      "wait: 1\n",
      "2132/2132 - 211s - loss: 0.0020 - r2: 0.9652 - val_loss: 0.0016 - val_r2: 0.9720 - 211s/epoch - 99ms/step\n",
      "Epoch 17/200\n",
      "wait: 2\n",
      "2132/2132 - 203s - loss: 0.0019 - r2: 0.9661 - val_loss: 0.0015 - val_r2: 0.9748 - 203s/epoch - 95ms/step\n",
      "Epoch 18/200\n",
      "2132/2132 - 171s - loss: 0.0019 - r2: 0.9667 - val_loss: 0.0014 - val_r2: 0.9749 - 171s/epoch - 80ms/step\n",
      "Epoch 19/200\n",
      "wait: 1\n",
      "2132/2132 - 127s - loss: 0.0018 - r2: 0.9676 - val_loss: 0.0014 - val_r2: 0.9765 - 127s/epoch - 60ms/step\n",
      "Epoch 20/200\n",
      "2132/2132 - 133s - loss: 0.0018 - r2: 0.9679 - val_loss: 0.0013 - val_r2: 0.9775 - 133s/epoch - 62ms/step\n",
      "Epoch 21/200\n",
      "wait: 1\n",
      "2132/2132 - 127s - loss: 0.0017 - r2: 0.9692 - val_loss: 0.0014 - val_r2: 0.9759 - 127s/epoch - 59ms/step\n",
      "Epoch 22/200\n",
      "2132/2132 - 133s - loss: 0.0017 - r2: 0.9695 - val_loss: 0.0013 - val_r2: 0.9777 - 133s/epoch - 62ms/step\n",
      "Epoch 23/200\n",
      "2132/2132 - 140s - loss: 0.0017 - r2: 0.9699 - val_loss: 0.0013 - val_r2: 0.9789 - 140s/epoch - 66ms/step\n",
      "Epoch 24/200\n",
      "2132/2132 - 134s - loss: 0.0017 - r2: 0.9701 - val_loss: 0.0012 - val_r2: 0.9790 - 134s/epoch - 63ms/step\n",
      "Epoch 25/200\n",
      "2132/2132 - 138s - loss: 0.0016 - r2: 0.9707 - val_loss: 0.0012 - val_r2: 0.9793 - 138s/epoch - 65ms/step\n",
      "Epoch 26/200\n",
      "2132/2132 - 135s - loss: 0.0016 - r2: 0.9707 - val_loss: 0.0010 - val_r2: 0.9807 - 135s/epoch - 63ms/step\n",
      "Epoch 27/200\n",
      "wait: 1\n",
      "2132/2132 - 134s - loss: 0.0016 - r2: 0.9716 - val_loss: 0.0012 - val_r2: 0.9770 - 134s/epoch - 63ms/step\n",
      "Epoch 28/200\n",
      "wait: 2\n",
      "2132/2132 - 134s - loss: 0.0015 - r2: 0.9716 - val_loss: 0.0011 - val_r2: 0.9805 - 134s/epoch - 63ms/step\n",
      "Epoch 29/200\n",
      "wait: 3\n",
      "2132/2132 - 135s - loss: 0.0016 - r2: 0.9716 - val_loss: 0.0011 - val_r2: 0.9809 - 135s/epoch - 63ms/step\n",
      "Epoch 30/200\n",
      "2132/2132 - 136s - loss: 0.0015 - r2: 0.9728 - val_loss: 0.0010 - val_r2: 0.9814 - 136s/epoch - 64ms/step\n",
      "Epoch 31/200\n",
      "wait: 1\n",
      "2132/2132 - 136s - loss: 0.0015 - r2: 0.9735 - val_loss: 0.0010 - val_r2: 0.9811 - 136s/epoch - 64ms/step\n",
      "Epoch 32/200\n",
      "wait: 2\n",
      "2132/2132 - 137s - loss: 0.0014 - r2: 0.9735 - val_loss: 0.0010 - val_r2: 0.9805 - 137s/epoch - 64ms/step\n",
      "Epoch 33/200\n",
      "wait: 3\n",
      "2132/2132 - 131s - loss: 0.0015 - r2: 0.9725 - val_loss: 0.0010 - val_r2: 0.9818 - 131s/epoch - 61ms/step\n",
      "Epoch 34/200\n",
      "wait: 4\n",
      "2132/2132 - 130s - loss: 0.0015 - r2: 0.9736 - val_loss: 0.0010 - val_r2: 0.9814 - 130s/epoch - 61ms/step\n",
      "Epoch 35/200\n",
      "wait: 5\n",
      "2132/2132 - 131s - loss: 0.0015 - r2: 0.9735 - val_loss: 0.0011 - val_r2: 0.9807 - 131s/epoch - 62ms/step\n",
      "Epoch 36/200\n",
      "2132/2132 - 133s - loss: 0.0014 - r2: 0.9745 - val_loss: 9.5944e-04 - val_r2: 0.9827 - 133s/epoch - 62ms/step\n",
      "Epoch 37/200\n",
      "wait: 1\n",
      "2132/2132 - 131s - loss: 0.0014 - r2: 0.9747 - val_loss: 9.9370e-04 - val_r2: 0.9820 - 131s/epoch - 62ms/step\n",
      "Epoch 38/200\n",
      "wait: 2\n",
      "2132/2132 - 132s - loss: 0.0014 - r2: 0.9748 - val_loss: 0.0010 - val_r2: 0.9815 - 132s/epoch - 62ms/step\n",
      "Epoch 39/200\n",
      "2132/2132 - 132s - loss: 0.0014 - r2: 0.9749 - val_loss: 9.5021e-04 - val_r2: 0.9833 - 132s/epoch - 62ms/step\n",
      "Epoch 40/200\n",
      "2132/2132 - 137s - loss: 0.0013 - r2: 0.9756 - val_loss: 9.4703e-04 - val_r2: 0.9828 - 137s/epoch - 64ms/step\n",
      "Epoch 41/200\n",
      "wait: 1\n",
      "2132/2132 - 136s - loss: 0.0013 - r2: 0.9757 - val_loss: 0.0010 - val_r2: 0.9813 - 136s/epoch - 64ms/step\n",
      "Epoch 42/200\n",
      "wait: 2\n",
      "2132/2132 - 138s - loss: 0.0013 - r2: 0.9755 - val_loss: 9.6607e-04 - val_r2: 0.9825 - 138s/epoch - 65ms/step\n",
      "Epoch 43/200\n",
      "2132/2132 - 136s - loss: 0.0013 - r2: 0.9760 - val_loss: 9.1031e-04 - val_r2: 0.9832 - 136s/epoch - 64ms/step\n",
      "Epoch 44/200\n",
      "wait: 1\n",
      "2132/2132 - 133s - loss: 0.0013 - r2: 0.9764 - val_loss: 9.5309e-04 - val_r2: 0.9827 - 133s/epoch - 62ms/step\n",
      "Epoch 45/200\n",
      "wait: 2\n",
      "2132/2132 - 138s - loss: 0.0013 - r2: 0.9764 - val_loss: 9.7680e-04 - val_r2: 0.9832 - 138s/epoch - 65ms/step\n",
      "Epoch 46/200\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          epochs=200,\n",
    "          verbose=2,\n",
    "          validation_data=val_data,\n",
    "          callbacks=[t_earlystopper, t_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --t_logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
