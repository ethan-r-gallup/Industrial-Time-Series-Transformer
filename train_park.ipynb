{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from einops import rearrange, repeat\n",
    "from ToApps.to_apps import to_slack\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import logical_not\n",
    "from builderfuncs import build_transformer, save_whole_model, restore_model\n",
    "from builderfuncs import restore_model, EarlyStopAndSave\n",
    "from parameterdicts import TransformerParameters\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/scaled_U2_data.csv', index_col=0)\n",
    "df.drop(\"UNNAMED: 0\", axis=1, inplace=True)\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(ds: tf.data.Dataset, ds_size: int, \n",
    "              train_split: float = 0.8, \n",
    "              val_split: float = 0.1, \n",
    "              test_split: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "                                                tf.data.Dataset, \n",
    "                                                tf.data.Dataset]:\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "   \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def mask_window(features: Tuple[Dict[tf.Tensor, tf.Tensor], tf.Tensor], \n",
    "                mask: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor], \n",
    "                                          tf.Tensor]:\n",
    "    inputs = features[0]\n",
    "    labels = features[1]\n",
    "    decoder_labels = tf.einsum('tf,t->tf', labels, mask)\n",
    "    mask = repeat(mask, 'i -> 1 i j', j=labels.shape[-2])\n",
    "    mask = tf.math.minimum(mask, rearrange(mask, '1 i j -> 1 j i'))\n",
    "    decoder_labels.set_shape([12, None])\n",
    "    mask.set_shape([None, 12, 12])\n",
    "    inputs.update({'decoder_labels':decoder_labels,\n",
    "                   'attention_mask':mask})\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def split_window(features: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor],\n",
    "                                               tf.Tensor]:\n",
    "    encoder_input = features[:12, 1:]\n",
    "    decoder_input = features[12:, 1:75]\n",
    "    paddings = tf.constant([[0, 0,], [0, 173]])\n",
    "    encoder_input = tf.concat([encoder_input, tf.pad(decoder_input, paddings, \"CONSTANT\")], axis=0)\n",
    "    labels = rearrange(features[11:, 0], 'a -> a 1')\n",
    "    decoder_input = features[11:, 1:75]\n",
    "    encoder_input.set_shape([23, 247])\n",
    "    decoder_input.set_shape([12, 74])\n",
    "    labels.set_shape([12, None])\n",
    "    \n",
    "\n",
    "    return {'encoder_inputs':encoder_input, 'decoder_inputs':decoder_input}, labels\n",
    "\n",
    "                        \n",
    "def make_dataset(data: pd.DataFrame, length: int, \n",
    "                 batch_size: int = 64, multistep: bool = True) -> Tuple[tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset]:\n",
    "    data = np.array(data.iloc[:, :], dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                        targets=None,\n",
    "                                                        sequence_length=length,\n",
    "                                                        sequence_stride=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=1,\n",
    "                                                        batch_size=None)\n",
    "\n",
    "    ds = ds.filter(lambda x: tf.reduce_all(logical_not(tf.math.is_nan(x))))\n",
    "    ds = ds.map(split_window)\n",
    "    if multistep == True:\n",
    "        nums = np.zeros(170680)\n",
    "    else:\n",
    "        nums = np.random.randint(0, length/2-1, 170680)\n",
    "    mask = tf.sequence_mask(nums, length/2, dtype=tf.float32)\n",
    "    maskds = tf.data.Dataset.from_tensor_slices(mask)\n",
    "    ds = tf.data.Dataset.zip((ds, maskds))\n",
    "    ds = ds.map(mask_window).batch(batch_size)\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(170680//batch_size + 1))\n",
    "    train_ds, val_ds, test_ds = ttv_split(ds, 170680//batch_size, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "    train_ds = train_ds\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "park_tensorboard_callback = tf.keras.callbacks.TensorBoard(park_logdir, histogram_freq=1)\n",
    "park_earlystopper = EarlyStopAndSave(filepath=\"model_folder/park_folder\", patience=15, quickstop=\"val_r2\")\n",
    "train_data, val_data, test_data = make_dataset(df, 23, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters: TransformerParameters = {\n",
    "\t\"look_back\": 23,\n",
    "\t\"n_features\": 247,\n",
    "    \"n_manips\": 74,\n",
    "    \"n_targs\": 1,\n",
    "\t\"horizon\": 12,\n",
    "\n",
    "\t\"key_dim\": 62,\n",
    "\t\"num_heads\": 4,\n",
    "\t\"ff_dim\": 10,\n",
    "\t\"ff_activ\": \"tanh\",\n",
    "\t\"num_encoders\": 2,\n",
    "    \"num_decoders\": 0,\n",
    "\t\"mlp_layers\": 2,\n",
    "\t\"mlp_units\": 256,\n",
    "\t\"mlp_dropout\": 0.2,\n",
    "\t\"mlp_activ\": \"linear\",\n",
    "\t\"dropout\": 0.2,\n",
    "\t\"out_activ\": \"linear\",\n",
    "\n",
    "\t\"learning_rate\": 0.001,\n",
    "\t\"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.98,\n",
    "    \"epsilon\": 1e-9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer(parameters=parameters, name=\"park_transformer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2132/2132 [==============================] - 53s 23ms/step - loss: 0.0199 - r2: 0.5804 - val_loss: 0.0111 - val_r2: 0.7808\n",
      "Epoch 2/200\n",
      "2132/2132 [==============================] - 48s 23ms/step - loss: 0.0120 - r2: 0.7605 - val_loss: 0.0111 - val_r2: 0.7828\n",
      "Epoch 3/200\n",
      "2132/2132 [==============================] - 50s 23ms/step - loss: 0.0116 - r2: 0.7683 - val_loss: 0.0106 - val_r2: 0.7936\n",
      "Epoch 4/200\n",
      "2129/2132 [============================>.] - ETA: 0s - loss: 0.0114 - r2: 0.7717wait: 1\n",
      "2132/2132 [==============================] - 49s 23ms/step - loss: 0.0114 - r2: 0.7717 - val_loss: 0.0109 - val_r2: 0.7914\n",
      "Epoch 5/200\n",
      "2132/2132 [==============================] - 48s 23ms/step - loss: 0.0114 - r2: 0.7721 - val_loss: 0.0105 - val_r2: 0.7951\n",
      "Epoch 6/200\n",
      "2129/2132 [============================>.] - ETA: 0s - loss: 0.0113 - r2: 0.7740wait: 1\n",
      "2132/2132 [==============================] - 51s 24ms/step - loss: 0.0113 - r2: 0.7740 - val_loss: 0.0114 - val_r2: 0.7825\n",
      "Epoch 7/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0113 - r2: 0.7747wait: 2\n",
      "2132/2132 [==============================] - 52s 24ms/step - loss: 0.0113 - r2: 0.7747 - val_loss: 0.0112 - val_r2: 0.7883\n",
      "Epoch 8/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0112 - r2: 0.7755wait: 3\n",
      "2132/2132 [==============================] - 51s 24ms/step - loss: 0.0112 - r2: 0.7755 - val_loss: 0.0119 - val_r2: 0.7727\n",
      "Epoch 9/200\n",
      "2132/2132 [==============================] - 57s 27ms/step - loss: 0.0112 - r2: 0.7768 - val_loss: 0.0105 - val_r2: 0.7897\n",
      "Epoch 10/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0113 - r2: 0.7763wait: 1\n",
      "2132/2132 [==============================] - 56s 26ms/step - loss: 0.0113 - r2: 0.7762 - val_loss: 0.0115 - val_r2: 0.7802\n",
      "Epoch 11/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0112 - r2: 0.7767wait: 2\n",
      "2132/2132 [==============================] - 53s 25ms/step - loss: 0.0112 - r2: 0.7766 - val_loss: 0.0114 - val_r2: 0.7780\n",
      "Epoch 12/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0111 - r2: 0.7810wait: 3\n",
      "2132/2132 [==============================] - 54s 25ms/step - loss: 0.0111 - r2: 0.7810 - val_loss: 0.0112 - val_r2: 0.7697\n",
      "Epoch 13/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0111 - r2: 0.7781wait: 4\n",
      "2132/2132 [==============================] - 55s 26ms/step - loss: 0.0111 - r2: 0.7781 - val_loss: 0.0108 - val_r2: 0.7890\n",
      "Epoch 14/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7790wait: 5\n",
      "2132/2132 [==============================] - 63s 29ms/step - loss: 0.0111 - r2: 0.7789 - val_loss: 0.0108 - val_r2: 0.7951\n",
      "Epoch 15/200\n",
      "2132/2132 [==============================] - 69s 32ms/step - loss: 0.0111 - r2: 0.7792 - val_loss: 0.0104 - val_r2: 0.7946\n",
      "Epoch 16/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7810wait: 1\n",
      "2132/2132 [==============================] - 69s 32ms/step - loss: 0.0111 - r2: 0.7810 - val_loss: 0.0106 - val_r2: 0.7926\n",
      "Epoch 17/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7811wait: 2\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0111 - r2: 0.7811 - val_loss: 0.0108 - val_r2: 0.7921\n",
      "Epoch 18/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7792wait: 3\n",
      "2132/2132 [==============================] - 86s 40ms/step - loss: 0.0111 - r2: 0.7791 - val_loss: 0.0110 - val_r2: 0.7782\n",
      "Epoch 19/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7798wait: 4\n",
      "2132/2132 [==============================] - 67s 31ms/step - loss: 0.0110 - r2: 0.7798 - val_loss: 0.0128 - val_r2: 0.7572\n",
      "Epoch 20/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7800wait: 5\n",
      "2132/2132 [==============================] - 68s 32ms/step - loss: 0.0111 - r2: 0.7800 - val_loss: 0.0110 - val_r2: 0.7936\n",
      "Epoch 21/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7804wait: 6\n",
      "2132/2132 [==============================] - 70s 33ms/step - loss: 0.0111 - r2: 0.7804 - val_loss: 0.0116 - val_r2: 0.7693\n",
      "Epoch 22/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7805wait: 7\n",
      "2132/2132 [==============================] - 73s 34ms/step - loss: 0.0111 - r2: 0.7804 - val_loss: 0.0130 - val_r2: 0.7551\n",
      "Epoch 23/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7794wait: 8\n",
      "2132/2132 [==============================] - 71s 33ms/step - loss: 0.0110 - r2: 0.7794 - val_loss: 0.0109 - val_r2: 0.7942\n",
      "Epoch 24/200\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0104 - val_r2: 0.7955\n",
      "Epoch 25/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0111 - r2: 0.7805wait: 1\n",
      "2132/2132 [==============================] - 73s 34ms/step - loss: 0.0111 - r2: 0.7805 - val_loss: 0.0108 - val_r2: 0.7962\n",
      "Epoch 26/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7820wait: 2\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0107 - val_r2: 0.7855\n",
      "Epoch 27/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7808wait: 3\n",
      "2132/2132 [==============================] - 75s 35ms/step - loss: 0.0111 - r2: 0.7808 - val_loss: 0.0105 - val_r2: 0.7922\n",
      "Epoch 28/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7801wait: 4\n",
      "2132/2132 [==============================] - 76s 36ms/step - loss: 0.0110 - r2: 0.7801 - val_loss: 0.0110 - val_r2: 0.7870\n",
      "Epoch 29/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7817wait: 5\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7817 - val_loss: 0.0111 - val_r2: 0.7797\n",
      "Epoch 30/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7815wait: 6\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0110 - r2: 0.7814 - val_loss: 0.0112 - val_r2: 0.7894\n",
      "Epoch 31/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0111 - r2: 0.7804wait: 7\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0111 - r2: 0.7804 - val_loss: 0.0106 - val_r2: 0.7953\n",
      "Epoch 32/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7809wait: 8\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0110 - r2: 0.7809 - val_loss: 0.0105 - val_r2: 0.7964\n",
      "Epoch 33/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7800wait: 9\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0110 - r2: 0.7800 - val_loss: 0.0108 - val_r2: 0.7880\n",
      "Epoch 34/200\n",
      "2132/2132 [==============================] - 75s 35ms/step - loss: 0.0110 - r2: 0.7812 - val_loss: 0.0103 - val_r2: 0.7976\n",
      "Epoch 35/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7804wait: 1\n",
      "2132/2132 [==============================] - 75s 35ms/step - loss: 0.0110 - r2: 0.7803 - val_loss: 0.0114 - val_r2: 0.7858\n",
      "Epoch 36/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7820wait: 2\n",
      "2132/2132 [==============================] - 80s 38ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0112 - val_r2: 0.7901\n",
      "Epoch 37/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7813wait: 3\n",
      "2132/2132 [==============================] - 78s 36ms/step - loss: 0.0110 - r2: 0.7813 - val_loss: 0.0105 - val_r2: 0.7974\n",
      "Epoch 38/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7821wait: 4\n",
      "2132/2132 [==============================] - 78s 37ms/step - loss: 0.0110 - r2: 0.7821 - val_loss: 0.0106 - val_r2: 0.7926\n",
      "Epoch 39/200\n",
      "2132/2132 [==============================] - 79s 37ms/step - loss: 0.0111 - r2: 0.7811 - val_loss: 0.0103 - val_r2: 0.7980\n",
      "Epoch 40/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7818wait: 1\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7818 - val_loss: 0.0106 - val_r2: 0.7989\n",
      "Epoch 41/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7814wait: 2\n",
      "2132/2132 [==============================] - 79s 37ms/step - loss: 0.0110 - r2: 0.7814 - val_loss: 0.0108 - val_r2: 0.7808\n",
      "Epoch 42/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7805wait: 3\n",
      "2132/2132 [==============================] - 77s 36ms/step - loss: 0.0110 - r2: 0.7804 - val_loss: 0.0126 - val_r2: 0.7672\n",
      "Epoch 43/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7816wait: 4\n",
      "2132/2132 [==============================] - 80s 38ms/step - loss: 0.0110 - r2: 0.7816 - val_loss: 0.0109 - val_r2: 0.7900\n",
      "Epoch 44/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7810wait: 5\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7810 - val_loss: 0.0109 - val_r2: 0.7895\n",
      "Epoch 45/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7814wait: 6\n",
      "2132/2132 [==============================] - 71s 33ms/step - loss: 0.0110 - r2: 0.7814 - val_loss: 0.0113 - val_r2: 0.7834\n",
      "Epoch 46/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7814wait: 7\n",
      "2132/2132 [==============================] - 70s 33ms/step - loss: 0.0110 - r2: 0.7814 - val_loss: 0.0104 - val_r2: 0.8026\n",
      "Epoch 47/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7811wait: 8\n",
      "2132/2132 [==============================] - 69s 33ms/step - loss: 0.0110 - r2: 0.7811 - val_loss: 0.0103 - val_r2: 0.7976\n",
      "Epoch 48/200\n",
      "2132/2132 [==============================] - 71s 33ms/step - loss: 0.0110 - r2: 0.7822 - val_loss: 0.0103 - val_r2: 0.7961\n",
      "Epoch 49/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7816wait: 1\n",
      "2132/2132 [==============================] - 70s 33ms/step - loss: 0.0110 - r2: 0.7816 - val_loss: 0.0106 - val_r2: 0.7954\n",
      "Epoch 50/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7814wait: 2\n",
      "2132/2132 [==============================] - 76s 35ms/step - loss: 0.0110 - r2: 0.7815 - val_loss: 0.0109 - val_r2: 0.7972\n",
      "Epoch 51/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7822wait: 3\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7822 - val_loss: 0.0113 - val_r2: 0.7837\n",
      "Epoch 52/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0109 - r2: 0.7833wait: 4\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0109 - r2: 0.7833 - val_loss: 0.0106 - val_r2: 0.7973\n",
      "Epoch 53/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7817wait: 5\n",
      "2132/2132 [==============================] - 71s 33ms/step - loss: 0.0110 - r2: 0.7817 - val_loss: 0.0107 - val_r2: 0.7968\n",
      "Epoch 54/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7816wait: 6\n",
      "2132/2132 [==============================] - 73s 34ms/step - loss: 0.0110 - r2: 0.7817 - val_loss: 0.0116 - val_r2: 0.7809\n",
      "Epoch 55/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7819wait: 7\n",
      "2132/2132 [==============================] - 77s 36ms/step - loss: 0.0110 - r2: 0.7818 - val_loss: 0.0121 - val_r2: 0.7750\n",
      "Epoch 56/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7820wait: 8\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0104 - val_r2: 0.8013\n",
      "Epoch 57/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7814wait: 9\n",
      "2132/2132 [==============================] - 75s 35ms/step - loss: 0.0110 - r2: 0.7814 - val_loss: 0.0105 - val_r2: 0.7970\n",
      "Epoch 58/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7837wait: 10\n",
      "2132/2132 [==============================] - 76s 36ms/step - loss: 0.0110 - r2: 0.7838 - val_loss: 0.0104 - val_r2: 0.7960\n",
      "Epoch 59/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0110 - r2: 0.7820wait: 11\n",
      "2132/2132 [==============================] - 74s 34ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0103 - val_r2: 0.8009\n",
      "Epoch 60/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7808wait: 12\n",
      "2132/2132 [==============================] - 74s 35ms/step - loss: 0.0110 - r2: 0.7808 - val_loss: 0.0109 - val_r2: 0.7883\n",
      "Epoch 61/200\n",
      "2132/2132 [==============================] - ETA: 0s - loss: 0.0109 - r2: 0.7833wait: 13\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0109 - r2: 0.7833 - val_loss: 0.0103 - val_r2: 0.7992\n",
      "Epoch 62/200\n",
      "2130/2132 [============================>.] - ETA: 0s - loss: 0.0109 - r2: 0.7829wait: 14\n",
      "2132/2132 [==============================] - 72s 34ms/step - loss: 0.0109 - r2: 0.7830 - val_loss: 0.0109 - val_r2: 0.7861\n",
      "Epoch 63/200\n",
      "2131/2132 [============================>.] - ETA: 0s - loss: 0.0110 - r2: 0.7820wait: 15\n",
      "2132/2132 [==============================] - 70s 33ms/step - loss: 0.0110 - r2: 0.7820 - val_loss: 0.0106 - val_r2: 0.7965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce237e70d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=val_data,\n",
    "          callbacks=[park_earlystopper, park_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builderfuncs import save_whole_model\n",
    "save_whole_model(model, \"model_folder/park_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --park_logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
