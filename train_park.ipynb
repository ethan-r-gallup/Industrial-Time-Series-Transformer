{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from einops import rearrange, repeat\n",
    "from ToApps.to_apps import to_slack\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import logical_not\n",
    "from ktfuncs import kt_transformer\n",
    "from builderfuncs import restore_model, EarlyStopAndSave\n",
    "from builderfuncs import build_transformer, save_whole_model, restore_model\n",
    "from parameterdicts import TransformerParameters\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/scaled_U2_data.csv', index_col=0)\n",
    "df.drop(\"UNNAMED: 0\", axis=1, inplace=True)\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(ds: tf.data.Dataset, ds_size: int, \n",
    "              train_split: float = 0.8, \n",
    "              val_split: float = 0.1, \n",
    "              test_split: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "                                                tf.data.Dataset, \n",
    "                                                tf.data.Dataset]:\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "   \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def mask_window(features: Tuple[Dict[tf.Tensor, tf.Tensor], tf.Tensor], \n",
    "                mask: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor], \n",
    "                                          tf.Tensor]:\n",
    "    inputs = features[0]\n",
    "    labels = features[1]\n",
    "    decoder_labels = tf.einsum('tf,t->tf', labels, mask)\n",
    "    mask = repeat(mask, 'i -> 1 i j', j=labels.shape[-2])\n",
    "    mask = tf.math.minimum(mask, rearrange(mask, '1 i j -> 1 j i'))\n",
    "    decoder_labels.set_shape([12, None])\n",
    "    mask.set_shape([None, 12, 12])\n",
    "    inputs.update({'decoder_labels':decoder_labels,\n",
    "                   'attention_mask':mask})\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def split_window(features: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor],\n",
    "                                               tf.Tensor]:\n",
    "    encoder_input = features[:12, 1:]\n",
    "    decoder_input = features[12:, 1:75]\n",
    "    paddings = tf.constant([[0, 0,], [0, 173]])\n",
    "    encoder_input = tf.concat([encoder_input, tf.pad(decoder_input, paddings, \"CONSTANT\")], axis=0)\n",
    "    labels = rearrange(features[11:, 0], 'a -> a 1')\n",
    "    decoder_input = features[11:, 1:75]\n",
    "    encoder_input.set_shape([23, 247])\n",
    "    decoder_input.set_shape([12, 74])\n",
    "    labels.set_shape([12, None])\n",
    "    \n",
    "\n",
    "    return {'encoder_inputs':encoder_input, 'decoder_inputs':decoder_input}, labels\n",
    "\n",
    "                        \n",
    "def make_dataset(data: pd.DataFrame, length: int, \n",
    "                 batch_size: int = 64, multistep: bool = True) -> Tuple[tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset]:\n",
    "    data = np.array(data.iloc[:, :], dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                        targets=None,\n",
    "                                                        sequence_length=length,\n",
    "                                                        sequence_stride=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=1,\n",
    "                                                        batch_size=None)\n",
    "\n",
    "    ds = ds.filter(lambda x: tf.reduce_all(logical_not(tf.math.is_nan(x))))\n",
    "    ds = ds.map(split_window)\n",
    "    if multistep == True:\n",
    "        nums = np.zeros(170680)\n",
    "    else:\n",
    "        nums = np.random.randint(0, length/2-1, 170680)\n",
    "    mask = tf.sequence_mask(nums, length/2, dtype=tf.float32)\n",
    "    maskds = tf.data.Dataset.from_tensor_slices(mask)\n",
    "    ds = tf.data.Dataset.zip((ds, maskds))\n",
    "    ds = ds.map(mask_window).batch(batch_size)\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(170680//batch_size + 1))\n",
    "    train_ds, val_ds, test_ds = ttv_split(ds, 170680//batch_size, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "    train_ds = train_ds\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_logdir = os.path.join(\"logs\", \"park-logs\")\n",
    "t_tensorboard_callback = tf.keras.callbacks.TensorBoard(t_logdir, histogram_freq=1)\n",
    "t_earlystopper = EarlyStopAndSave(filepath=\"model_folder/park_folder\", patience=15, quickstop=\"val_r2\")\n",
    "train_data, val_data, test_data = make_dataset(df, 23, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(kt_transformer,\n",
    "                                objective=kt.Objective('val_loss', 'min'),\n",
    "                                max_trials=200,\n",
    "                                directory='directory',\n",
    "                                project_name=\"park_trial_1\",\n",
    "                                seed=1)\n",
    "\n",
    "parameters = tuner.get_best_hyperparameters()[0].values\n",
    "parameters['look_back'] = parameters.pop('lookback')\n",
    "parameters['key_dim'] = 248//parameters['num_heads']\n",
    "model = build_transformer(parameters=parameters, name=\"park_model\", dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data,\n",
    "          epochs=200,\n",
    "          verbose=2,\n",
    "          validation_data=val_data,\n",
    "          callbacks=[t_earlystopper, t_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builderfuncs import save_whole_model\n",
    "save_whole_model(model, \"model_folder/park_folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
