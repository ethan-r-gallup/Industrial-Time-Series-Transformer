{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from einops import rearrange, repeat\n",
    "from ToApps.to_apps import to_slack\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import logical_not\n",
    "from builderfuncs import build_transformer, save_whole_model, restore_model\n",
    "from builderfuncs import restore_model, EarlyStopAndSave\n",
    "from parameterdicts import TransformerParameters\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/scaled_U2_data.csv', index_col=0)\n",
    "df.drop(\"UNNAMED: 0\", axis=1, inplace=True)\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(ds: tf.data.Dataset, ds_size: int, \n",
    "              train_split: float = 0.8, \n",
    "              val_split: float = 0.1, \n",
    "              test_split: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "                                                tf.data.Dataset, \n",
    "                                                tf.data.Dataset]:\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "   \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def mask_window(features: Tuple[Dict[tf.Tensor, tf.Tensor], tf.Tensor], \n",
    "                mask: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor], \n",
    "                                          tf.Tensor]:\n",
    "    inputs = features[0]\n",
    "    labels = features[1]\n",
    "    decoder_labels = tf.einsum('tf,t->tf', labels, mask)\n",
    "    mask = repeat(mask, 'i -> 1 i j', j=labels.shape[-2])\n",
    "    mask = tf.math.minimum(mask, rearrange(mask, '1 i j -> 1 j i'))\n",
    "    decoder_labels.set_shape([12, None])\n",
    "    mask.set_shape([None, 12, 12])\n",
    "    inputs.update({'decoder_labels':decoder_labels,\n",
    "                   'attention_mask':mask})\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def split_window(features: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor],\n",
    "                                               tf.Tensor]:\n",
    "    encoder_input = features[:12, 1:]\n",
    "    decoder_input = features[12:, 1:75]\n",
    "    paddings = tf.constant([[0, 0,], [0, 173]])\n",
    "    encoder_input = tf.concat([encoder_input, tf.pad(decoder_input, paddings, \"CONSTANT\")], axis=0)\n",
    "    labels = rearrange(features[11:, 0], 'a -> a 1')\n",
    "    decoder_input = features[11:, 1:75]\n",
    "    encoder_input.set_shape([23, 247])\n",
    "    decoder_input.set_shape([12, 74])\n",
    "    labels.set_shape([12, None])\n",
    "    \n",
    "\n",
    "    return {'encoder_inputs':encoder_input, 'decoder_inputs':decoder_input}, labels\n",
    "\n",
    "                        \n",
    "def make_dataset(data: pd.DataFrame, length: int, \n",
    "                 batch_size: int = 64, multistep: bool = True) -> Tuple[tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset]:\n",
    "    data = np.array(data.iloc[:, :], dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                        targets=None,\n",
    "                                                        sequence_length=length,\n",
    "                                                        sequence_stride=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=1,\n",
    "                                                        batch_size=None)\n",
    "\n",
    "    ds = ds.filter(lambda x: tf.reduce_all(logical_not(tf.math.is_nan(x))))\n",
    "    ds = ds.map(split_window)\n",
    "    if multistep == True:\n",
    "        nums = np.zeros(170680)\n",
    "    else:\n",
    "        nums = np.random.randint(0, length/2-1, 170680)\n",
    "    mask = tf.sequence_mask(nums, length/2, dtype=tf.float32)\n",
    "    maskds = tf.data.Dataset.from_tensor_slices(mask)\n",
    "    ds = tf.data.Dataset.zip((ds, maskds))\n",
    "    ds = ds.map(mask_window).batch(batch_size)\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(170680//batch_size + 1))\n",
    "    train_ds, val_ds, test_ds = ttv_split(ds, 170680//batch_size, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "    train_ds = train_ds\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_logdir = os.path.join(\"logs\", 'park-logs')\n",
    "# park_tensorboard_callback = tf.keras.callbacks.TensorBoard(park_logdir, histogram_freq=1)\n",
    "# park_earlystopper = EarlyStopAndSave(filepath=\"model_folder/park_folder\", patience=15, quickstop=\"val_r2\")\n",
    "train_data, val_data, test_data = make_dataset(df, 23, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builderfuncs import save_whole_model\n",
    "from keras_tuner.engine import tuner_utils\n",
    "import GPUtil\n",
    "gpu = GPUtil.getGPUs()[0]\n",
    "from time import sleep\n",
    "from ToApps.to_apps import to_slack\n",
    "\n",
    "def _build_and_fit_model(self, trial, *args, **kwargs):\n",
    "    \"\"\"For AutoKeras to override.\n",
    "    DO NOT REMOVE this function. AutoKeras overrides the function to tune\n",
    "    tf.data preprocessing pipelines, preprocess the dataset to obtain\n",
    "    the input shape before building the model, adapt preprocessing layers,\n",
    "    and tune other fit_args and fit_kwargs.\n",
    "    Args:\n",
    "        trial: A `Trial` instance that contains the information needed to\n",
    "            run this trial. `Hyperparameters` can be accessed via\n",
    "            `trial.hyperparameters`.\n",
    "        *args: Positional arguments passed by `search`.\n",
    "        **kwargs: Keyword arguments passed by `search`.\n",
    "    Returns:\n",
    "        The fit history.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    string = f\"Park_trial_{trial.trial_id}\\n\"\n",
    "    string += \"------------------------------------------------\\n\"\n",
    "    if trial.hyperparameters.values:\n",
    "            for hp, value in trial.hyperparameters.values.items():\n",
    "                string += f\"{hp}: {value}\\n\"\n",
    "    \n",
    "    hp = trial.hyperparameters\n",
    "    model = self._try_build(hp)\n",
    "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
    "    tuner_utils.validate_trial_results(\n",
    "        results, self.oracle.objective, \"HyperModel.fit()\"\n",
    "    )\n",
    "    try:\n",
    "        path = os.path.join(\"parkdir2\", f\"trial_{trial.trial_id}\")\n",
    "        os.mkdir(path) \n",
    "    except FileExistsError:\n",
    "         pass\n",
    "    save_whole_model(model, f\"parkdir2/trial_{trial.trial_id}\")\n",
    "\n",
    "    string += \"------------------------------------------------\\n\"\n",
    "    result_dict = tuner_utils.convert_to_metrics_dict(results, self.oracle.objective)\n",
    "    string += f\"loss: {result_dict['loss']:.6f}, r2: {result_dict['r2']:.6f}\\n\"\n",
    "    string += f\"val_loss: {result_dict['val_loss']:.6f}, val_r2: {result_dict['val_r2']:.6f}\\n\"\n",
    "             \n",
    "    string += \"------------------------------------------------\\n\"\n",
    "    while GPUtil.getGPUs()[0].temperature > 89.0:\n",
    "        to_slack(str(GPUtil.getGPUs()[0].temperature))\n",
    "        sleep(10)\n",
    "    try:\n",
    "        to_slack(string)\n",
    "    except:\n",
    "         print(\"cannot connect to slack\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import keras_tuner as kt\n",
    "from ktfuncs import kt_park\n",
    "tuner = kt.BayesianOptimization(kt_park,\n",
    "                                objective=kt.Objective('val_loss', 'min'),\n",
    "                                max_trials=100,\n",
    "                                num_initial_points=54,\n",
    "                                directory='directory',\n",
    "                                project_name=\"park_trial_2\",\n",
    "                                seed=1,\n",
    "                                max_consecutive_failed_trials=1)\n",
    "tuner._build_and_fit_model = types.MethodType(_build_and_fit_model, tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 20\n",
      "num_heads (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 16, 'step': 2, 'sampling': 'linear'}\n",
      "lookback (Fixed)\n",
      "{'conditions': [], 'value': 23}\n",
      "n_features (Fixed)\n",
      "{'conditions': [], 'value': 247}\n",
      "n_manips (Fixed)\n",
      "{'conditions': [], 'value': 74}\n",
      "n_targs (Fixed)\n",
      "{'conditions': [], 'value': 1}\n",
      "horizon (Fixed)\n",
      "{'conditions': [], 'value': 12}\n",
      "ff_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 16, 'step': 4, 'sampling': 'linear'}\n",
      "ff_activ (Choice)\n",
      "{'default': 'linear', 'conditions': [], 'values': ['linear', 'relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "num_encoders (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': 'linear'}\n",
      "num_decoders (Fixed)\n",
      "{'conditions': [], 'value': 0}\n",
      "mlp_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "mlp_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': 'linear'}\n",
      "mlp_activ (Choice)\n",
      "{'default': 'linear', 'conditions': [], 'values': ['linear', 'relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "mlp_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.7, 'step': 0.1, 'sampling': 'linear'}\n",
      "dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.7, 'step': 0.1, 'sampling': 'linear'}\n",
      "out_activ (Choice)\n",
      "{'default': 'linear', 'conditions': [], 'values': ['linear', 'relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "learning_rate (Fixed)\n",
      "{'conditions': [], 'value': 0.0001}\n",
      "beta_1 (Fixed)\n",
      "{'conditions': [], 'value': 0.85}\n",
      "beta_2 (Float)\n",
      "{'default': 0.99, 'conditions': [], 'min_value': 0.99, 'max_value': 0.999, 'step': 0.001, 'sampling': 'linear'}\n",
      "epsilon (Fixed)\n",
      "{'conditions': [], 'value': 1e-07}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 Complete [00h 02m 45s]\n",
      "val_loss: 0.027632107958197594\n",
      "\n",
      "Best val_loss So Far: 0.0019061993807554245\n",
      "Total elapsed time: 17h 24m 23s\n",
      "\n",
      "Search: Running Trial #33\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "8                 |6                 |num_heads\n",
      "23                |23                |lookback\n",
      "247               |247               |n_features\n",
      "74                |74                |n_manips\n",
      "1                 |1                 |n_targs\n",
      "12                |12                |horizon\n",
      "16                |12                |ff_dim\n",
      "gelu              |tanh              |ff_activ\n",
      "1                 |2                 |num_encoders\n",
      "0                 |0                 |num_decoders\n",
      "1                 |4                 |mlp_layers\n",
      "64                |128               |mlp_units\n",
      "sigmoid           |gelu              |mlp_activ\n",
      "0.4               |0.3               |mlp_dropout\n",
      "0.5               |0.3               |dropout\n",
      "gelu              |gelu              |out_activ\n",
      "0.0001            |0.0001            |learning_rate\n",
      "0.85              |0.85              |beta_1\n",
      "0.997             |0.991             |beta_2\n",
      "1e-07             |1e-07             |epsilon\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_10788\\4106867355.py\", line 3, in <module>\n",
      "    tuner.search(train_data,\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 230, in search\n",
      "    self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_10788\\1510145302.py\", line 33, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 144, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1606, in fit\n",
      "    val_logs = self.evaluate(\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1947, in evaluate\n",
      "    tmp_logs = self.test_function(iterator)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 986, in _call\n",
      "    return self._concrete_stateful_fn._call_flat(\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1862, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 499, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"c:\\Users\\Owner\\anaconda3\\envs\\plant_proj_venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "try:\n",
    "    tuner.search(train_data,\n",
    "                epochs=25,\n",
    "                verbose=2,\n",
    "                validation_data=val_data,\n",
    "                callbacks=[EarlyStopAndSave(filepath=\"ffolder\", patience=3, quickstop=\"val_r2\", lim=0.5)])\n",
    "except:\n",
    "    # printing stack trace\n",
    "    for i in range(5):\n",
    "        to_slack(\"PARK ERROR\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "hyperparams = []\n",
    "for i in range(100):\n",
    "    try:\n",
    "        with open(f\"directory/park_trial_2/trial_{i:03}/trial.json\") as f:\n",
    "            parameters = json.load(f)\n",
    "        hyperparams.append(parameters)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"trial_{i:03} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(hyperparams):\n",
    "    if hyperparams[i][\"score\"] == None:\n",
    "        hyperparams.pop(i)\n",
    "        print(i)\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = sorted(hyperparams, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "from tabulate import tabulate\n",
    "for i in range(25):\n",
    "    line = [hyperparams[i]['trial_id']]\n",
    "    for key in list(hyperparams[i]['hyperparameters']['values'].keys()):\n",
    "        line.append(hyperparams[i]['hyperparameters']['values'][key])\n",
    "    line.append(hyperparams[i]['score'])\n",
    "    table.append(line)\n",
    "headers = list(hyperparams[i]['hyperparameters']['values'].keys())\n",
    "headers.append('score')\n",
    "print(tabulate(table, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = restore_model(\"parkdir/trial_052\", modeltype=\"transformer\")\n",
    "print(model.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_data,\n",
    "#           epochs=200,\n",
    "#           verbose=1,\n",
    "#           validation_data=val_data,\n",
    "#           callbacks=[park_earlystopper, park_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builderfuncs import save_whole_model\n",
    "# save_whole_model(model, \"model_folder/park_folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
