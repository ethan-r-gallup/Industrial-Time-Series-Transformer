{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import logical_not\n",
    "from builderfuncs import EarlyStopAndSave\n",
    "from builderfuncs import build_gru, build_bigru, build_bilstm, build_lstm, save_whole_model, restore_model\n",
    "from parameterdicts import GRUParameters\n",
    "from ktfuncs import *\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/scaled_U2_data.csv', index_col=0)\n",
    "df.drop(\"UNNAMED: 0\", axis=1, inplace=True)\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(ds: tf.data.Dataset, ds_size: int, \n",
    "              train_split: float = 0.8, \n",
    "              val_split: float = 0.1, \n",
    "              test_split: float = 0.1) -> Tuple[tf.data.Dataset, \n",
    "                                                tf.data.Dataset, \n",
    "                                                tf.data.Dataset]:\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "   \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def split_window(features: tf.Tensor) -> Tuple[Dict[tf.Tensor, tf.Tensor],\n",
    "                                               tf.Tensor]:\n",
    "    inputs = features[:12, 1:]\n",
    "    state_labels = features[12, 75:]\n",
    "    targ_labels = tf.expand_dims(features[11, 0], axis=0)\n",
    "    labels = tf.concat([targ_labels, state_labels], axis=0)\n",
    "    inputs.set_shape([12, 247])\n",
    "    labels.set_shape([174])\n",
    "    \n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "                        \n",
    "def make_dataset(data: pd.DataFrame, length: int, \n",
    "                 batch_size: int = 64, multistep: bool = True) -> Tuple[tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset, \n",
    "                                                                        tf.data.Dataset]:\n",
    "    data = np.array(data.iloc[:, :], dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(data=data,\n",
    "                                                        targets=None,\n",
    "                                                        sequence_length=length,\n",
    "                                                        sequence_stride=1,\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=1,\n",
    "                                                        batch_size=None)\n",
    "\n",
    "    ds = ds.filter(lambda x: tf.reduce_all(logical_not(tf.math.is_nan(x))))\n",
    "    ds = ds.map(split_window).batch(batch_size)\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(170680//batch_size + 1))\n",
    "    train_ds, val_ds, test_ds = ttv_split(ds, 170680//batch_size, train_split=0.8, val_split=0.1, test_split=0.1)\n",
    "    train_ds = train_ds\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_logdir = os.path.join(\"logs\", f'gru{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "gru_tensorboard_callback = tf.keras.callbacks.TensorBoard(gru_logdir, histogram_freq=1)\n",
    "gru_earlystopper = EarlyStopAndSave(filepath=\"model_folder/gru_folder\", patience=3, lim=0.005, minormax=\"max\")\n",
    "\n",
    "bigru_logdir = os.path.join(\"logs\", f'bigru{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "bigru_tensorboard_callback = tf.keras.callbacks.TensorBoard(bigru_logdir, histogram_freq=1)\n",
    "bigru_earlystopper = EarlyStopAndSave(filepath=\"model_folder/bigru_folder\", patience=3, lim=0.005, minormax=\"max\")\n",
    "\n",
    "lstm_logdir = os.path.join(\"logs\", f'lstm{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "lstm_tensorboard_callback = tf.keras.callbacks.TensorBoard(lstm_logdir, histogram_freq=1)\n",
    "lstm_earlystopper = EarlyStopAndSave(filepath=\"model_folder/lstm_folder\", patience=3, lim=0.005, minormax=\"max\")\n",
    "\n",
    "bilstm_logdir = os.path.join(\"logs\", f'bilstm{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "bilstm_tensorboard_callback = tf.keras.callbacks.TensorBoard(bilstm_logdir, histogram_freq=1)\n",
    "bilstm_earlystopper = EarlyStopAndSave(filepath=\"model_folder/bilstm_folder\", patience=3, lim=0.005, minormax=\"max\")\n",
    "\n",
    "train_data, val_data, test_data = make_dataset(df, 23, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_tuner = kt.BayesianOptimization(kt_gru,\n",
    "                                    objective=kt.Objective('val_loss', 'min'),\n",
    "                                    directory='directory',\n",
    "                                    project_name=\"gru_trial_1\",\n",
    "                                    seed=1)\n",
    "bigru_tuner = kt.BayesianOptimization(kt_bigru,\n",
    "                                    objective=kt.Objective('val_loss', 'min'),\n",
    "                                    directory='directory',\n",
    "                                    project_name=\"bigru_trial_1\",\n",
    "                                    seed=1)\n",
    "lstm_tuner = kt.BayesianOptimization(kt_lstm,\n",
    "                                    objective=kt.Objective('val_loss', 'min'),\n",
    "                                    directory='directory',\n",
    "                                    project_name=\"lstm_trial_1\",\n",
    "                                    seed=1)\n",
    "bilstm_tuner = kt.BayesianOptimization(kt_bilstm,\n",
    "                                    objective=kt.Objective('val_loss', 'min'),\n",
    "                                    directory='directory',\n",
    "                                    project_name=\"bilstm_trial_1\",\n",
    "                                    seed=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters: GRUParameters = gru_tuner.get_best_hyperparameters()[0].values\n",
    "gru = build_gru(parameters=parameters, name=\"gru_model\", dynamic=False)\n",
    "\n",
    "parameters: GRUParameters = bigru_tuner.get_best_hyperparameters()[0].values\n",
    "bigru = build_bigru(parameters=parameters, name=\"bigru_model\", dynamic=False)\n",
    "\n",
    "parameters: GRUParameters = lstm_tuner.get_best_hyperparameters()[0].values\n",
    "lstm = build_lstm(parameters=parameters, name=\"lstm_model\", dynamic=False)\n",
    "\n",
    "parameters: GRUParameters = bilstm_tuner.get_best_hyperparameters()[0].values\n",
    "bilstm = build_bilstm(parameters=parameters, name=\"bilstm_model\", dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.fit(train_data, epochs=200, verbose=2, validation_data=val_data, callbacks=[gru_earlystopper, gru_tensorboard_callback])\n",
    "bigru.fit(train_data, epochs=200, verbose=2, validation_data=val_data, callbacks=[bigru_earlystopper, bigru_tensorboard_callback])\n",
    "lstm.fit(train_data, epochs=200, verbose=2, validation_data=val_data, callbacks=[lstm_earlystopper, lstm_tensorboard_callback])\n",
    "bilstm.fit(train_data, epochs=200, verbose=2, validation_data=val_data, callbacks=[bilstm_earlystopper, bilstm_tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --gru_logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --bigru_logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --lstm_logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --bilstm_logdir logs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
